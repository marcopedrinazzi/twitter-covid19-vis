{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beneficial-proposition",
   "metadata": {},
   "source": [
    "# Tables - Fake Covid-19 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-running",
   "metadata": {},
   "source": [
    "## Tweets Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-mining",
   "metadata": {},
   "source": [
    "### tabella_fake and style.css"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-uruguay",
   "metadata": {},
   "source": [
    "We've used the following packages to create a table showing the Tweets and the relative Tweets links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-nutrition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import json\n",
    "from dateutil.parser import parse\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_table\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-dimension",
   "metadata": {},
   "source": [
    "We've defined a function to remove URLs from the Tweet's text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-georgia",
   "metadata": {},
   "source": [
    "In order to do the classification of the tweets, we need to read the CSV file and the JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dataframe = pd.read_csv('dataset/FINAL_fakecovid_final_filtered_dataset_clean.csv',sep=\";\")\n",
    "csv_dataframe['tweet_id'] = csv_dataframe['tweet_id'].astype(str)\n",
    "csv_list = csv_dataframe.values.tolist()\n",
    "lista_unica_csv=list(itertools.chain.from_iterable(csv_list))\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-timing",
   "metadata": {},
   "source": [
    "We're going to visualize Tweets and relative links in a table and we will classify them in two categories: \"fake\" and \"partially false\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "index= 0\n",
    "\n",
    "category = []\n",
    "date = []\n",
    "txt = []\n",
    "link = []\n",
    "\n",
    "for element in data:\n",
    "    token_id = data[index]['id_str']                          \n",
    "    indice_csv = lista_unica_csv.index(token_id)   \n",
    "    value_cat =  lista_unica_csv[indice_csv+1].lower()\n",
    "    if value_cat == \"false\":\n",
    "        value_cat = \"fake\"\n",
    "    category.append(value_cat.replace(\" \", \"\"))\n",
    "    \n",
    "    token=data[index]['created_at']\n",
    "    d = parse(token)\n",
    "    d = d.strftime('%Y/%m/%d')\n",
    "    date.append(d)\n",
    "    \n",
    "    txt.append(remove_urls(data[index]['full_text']))\n",
    "    link.append(\"[http://twitter.com/anyuser/status/\"+data[index]['id_str']+\"](http://twitter.com/anyuser/status/\"+data[index]['id_str']+\")\")\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-pressing",
   "metadata": {},
   "source": [
    "We create the Pandas DataFrame and then we work on it in order to create the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'Type': category,\n",
    "    'Date': date,\n",
    "    'Tweet': txt,\n",
    "    'Link': link\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-economy",
   "metadata": {},
   "source": [
    "In order to create the table, we've used the Dash module, that allows to generate an interactive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "#https://dash.plotly.com/datatable/filtering\n",
    "app.layout = html.Div([\n",
    "    dash_table.DataTable(\n",
    "        id='datatable-interactivity',\n",
    "        columns=[\n",
    "            {'name': 'Type', 'id': 'Type'},\n",
    "            {'name': 'Date', 'id': 'Date'},\n",
    "            {'name': 'Tweet', 'id': 'Tweet'},\n",
    "            {'name': 'Link', 'id':'Link', 'type': 'text', 'presentation':'markdown'}],\n",
    "        data=df.to_dict('records'),\n",
    "        style_filter={\n",
    "            \"backgroundColor\":\"white\"\n",
    "        },\n",
    "        style_data_conditional=[\n",
    "        {\n",
    "            'if': {\n",
    "                'column_id': 'Type',\n",
    "            },\n",
    "            'font-weight':'bold',\n",
    "            'width':'200px'\n",
    "        },\n",
    "        {\n",
    "            'if': {\n",
    "                'column_id': 'Date',\n",
    "            },\n",
    "            'width':'200px'\n",
    "        },\n",
    "        {\n",
    "            'if': {\n",
    "                'column_id': 'Tweet',\n",
    "            },\n",
    "            'width':'2500px'\n",
    "        },\n",
    "        {\n",
    "            'if': {\n",
    "                'column_id': 'Link',\n",
    "            },\n",
    "            'font-size':'16px'\n",
    "        }],\n",
    "        style_cell={\n",
    "            'textAlign':'left',\n",
    "            'font-family': 'Helvetica Neue',\n",
    "            'whiteSpace': 'normal',\n",
    "            'padding-bottom': '15px',\n",
    "            'border':'0px solid darkslategray',\n",
    "            'font-size':'16px',\n",
    "            'height': 'auto'\n",
    "        },\n",
    "        style_header={\n",
    "            'backgroundColor':\"white\", #mocassin\n",
    "            'font-family':'Helvetica Neue',\n",
    "            'font-weight': 'bold',\n",
    "            'whiteSpace': 'normal',\n",
    "            'padding': '10px',\n",
    "            'border-bottom':'1px solid darkslategray',\n",
    "            'font-size':'18px',\n",
    "            'height': 'auto'\n",
    "        },\n",
    "        style_data={\n",
    "            'whiteSpace': 'normal',\n",
    "            'height': 'auto'\n",
    "        },\n",
    "        filter_action=\"native\",\n",
    "        sort_action=\"native\",\n",
    "        sort_mode=\"multi\",\n",
    "        page_action=\"native\",\n",
    "        page_current= 0,\n",
    "        page_size= 8,\n",
    "        fill_width=False\n",
    "    ),\n",
    "    html.Div(id='datatable-interactivity-container')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-lounge",
   "metadata": {},
   "source": [
    "For the \"filter data\" field, unlike the other table cells, the style was applied using a CSS file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    ".dash-table-container .dash-spreadsheet-container .dash-spreadsheet-inner input:not([type=radio]):not([type=checkbox]){\n",
    "    color: black!important;\n",
    "    text-align: left!important;\n",
    "    font-family: 'Helvetica Neue'!important;\n",
    "    font-size: 16px!important;\n",
    "    padding: 20px!important;\n",
    "    font-weight: bold!important;\n",
    "}\n",
    "\n",
    "a {\n",
    "    color:black!important;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-tattoo",
   "metadata": {},
   "source": [
    "## Tweets Insider Links Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-position",
   "metadata": {},
   "source": [
    "### urlcount.py and tabella_urls_fake.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-duncan",
   "metadata": {},
   "source": [
    "We've used the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil.parser import parse\n",
    "import itertools\n",
    "import requests\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import dash_table\n",
    "import dash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-kennedy",
   "metadata": {},
   "source": [
    "In order to do the classification of the tweets, we need to read the CSV file and the JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dataframe = pd.read_csv('dataset/FINAL_fakecovid_final_filtered_dataset_clean.csv',sep=\";\")\n",
    "csv_dataframe['tweet_id'] = csv_dataframe['tweet_id'].astype(str)\n",
    "csv_list = csv_dataframe.values.tolist()\n",
    "lista_unica_csv=list(itertools.chain.from_iterable(csv_list))\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-mortgage",
   "metadata": {},
   "source": [
    "We have to extract the link from the Tweet, so with the BeautifulSoup module, we obtain the title of the page (which is located at the link indicated in the Tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "urls = []\n",
    "titles = []\n",
    "dates = []\n",
    "category = []\n",
    "for element in data:\n",
    "    print(index)\n",
    "    if data[index]['entities']['urls'] is not None:\n",
    "        for entity in data[index]['entities']['urls']:\n",
    "            #TIMEOUT\n",
    "            #https://stackoverflow.com/questions/16511337/correct-way-to-try-except-using-python-requests-module\n",
    "\n",
    "            # SERVONO DAVVERO SOLO QUESTI CAMPI?\n",
    "            if entity['expanded_url'].lower() not in urls:\n",
    "                token_id = data[index]['id_str']                          \n",
    "                indice_csv = lista_unica_csv.index(token_id)   \n",
    "                value_cat =  lista_unica_csv[indice_csv+1].lower()\n",
    "                if value_cat == \"false\":\n",
    "                    value_cat = \"fake\"\n",
    "                try:\n",
    "                    r = requests.get(entity['expanded_url'], timeout=10)\n",
    "                except requests.exceptions.Timeout:\n",
    "                    titles.append(\"[TIMEOUT ERROR]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                    urls.append(entity['expanded_url'].lower())\n",
    "                    category.append(value_cat.replace(\" \", \"\"))\n",
    "                    d = parse(data[index]['created_at'])\n",
    "                    d = d.strftime('%Y/%m/%d')\n",
    "                    dates.append(d)\n",
    "                except requests.ConnectionError:\n",
    "                    titles.append(\"[CONNECTION ERROR]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                    urls.append(entity['expanded_url'].lower())\n",
    "                    category.append(value_cat.replace(\" \", \"\"))\n",
    "                    d = parse(data[index]['created_at'])\n",
    "                    d = d.strftime('%Y/%m/%d')\n",
    "                    dates.append(d)\n",
    "                else:\n",
    "                    soup = BeautifulSoup(r.text,features=\"lxml\")\n",
    "                    if soup.title is None:\n",
    "                        titles.append(\"[NO TITLE ERROR]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                        urls.append(entity['expanded_url'].lower())\n",
    "                        category.append(value_cat.replace(\" \", \"\"))\n",
    "                        d = parse(data[index]['created_at'])\n",
    "                        d = d.strftime('%Y/%m/%d')\n",
    "                        dates.append(d)\n",
    "                    else:\n",
    "                        titles.append(\"[\"+soup.title.text+\"]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                        urls.append(entity['expanded_url'].lower())\n",
    "                        category.append(value_cat.replace(\" \", \"\"))\n",
    "                        d = parse(data[index]['created_at'])\n",
    "                        d = d.strftime('%Y/%m/%d')\n",
    "                        dates.append(d)\n",
    "            else:\n",
    "                print(\"URL già presente\")\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-venice",
   "metadata": {},
   "source": [
    "We create the Pandas DataFrame, in order to create a table, containing all the links in the Tweets, that it will be transcripted in a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-screen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'Type': category,\n",
    "    'Link': titles,\n",
    "    'First-Shared': dates\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-flood",
   "metadata": {},
   "source": [
    "Then, we create the CSV file, that we'll use to create the Dash DataTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('urls.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-garage",
   "metadata": {},
   "source": [
    "### Let's create the Dash DataTable...but first"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-discharge",
   "metadata": {},
   "source": [
    "The generated URLs dataset (urls.csv) contained links that referred to:\n",
    "- private Tweets, so not publicly visible and considered not relevant\n",
    "- Tweets of suspended accounts, so no longer available\n",
    "- deleted Tweets\n",
    "- web pages no longer available\n",
    "\n",
    "Thus, we have manually cleaned the dataset by veryfing every single link and by removing those rows that contained these irrilevant links."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-checklist",
   "metadata": {},
   "source": [
    "### Now we can start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-challenge",
   "metadata": {},
   "source": [
    "First, we have to read the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-contrary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('urls.csv',sep=';')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-desert",
   "metadata": {},
   "source": [
    "Then, we create the Dash DataTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "#https://dash.plotly.com/datatable/filtering\n",
    "app.layout = html.Div([\n",
    "    dash_table.DataTable(\n",
    "        id='datatable-interactivity',\n",
    "        columns=[{'name': 'Type', 'id':'Type'},\n",
    "            {'name': 'Link', 'id':'Link', 'type': 'text', 'presentation':'markdown'},\n",
    "            {'name': 'First-Shared', 'id': 'First-Shared'}],\n",
    "        data=df.to_dict('records'),\n",
    "        style_data_conditional=[{\n",
    "            'if': {\n",
    "                'column_id': 'Type',\n",
    "            },\n",
    "            'font-weight':'bold',\n",
    "            'width':'200px'\n",
    "        }],\n",
    "        style_filter={\n",
    "            \"backgroundColor\":\"white\"\n",
    "        },\n",
    "        style_cell={\n",
    "            'textAlign':'left',\n",
    "            'font-family': 'Helvetica Neue',\n",
    "            'border':'0px solid darkslategray',\n",
    "            'font-size':'16px',\n",
    "        },\n",
    "        style_header={\n",
    "            'backgroundColor':\"moccasin\", #moccasin\n",
    "            'font-family':'Helvetica Neue',\n",
    "            'font-weight': 'bold',\n",
    "            'border-bottom':'1px solid darkslategray',\n",
    "            'font-size':'18px',\n",
    "        },\n",
    "        filter_action=\"native\",\n",
    "        sort_action=\"native\",\n",
    "        sort_mode=\"multi\",\n",
    "        page_action=\"native\",\n",
    "        page_current= 0,\n",
    "        page_size= 12,\n",
    "    ),\n",
    "    html.Div(id='datatable-interactivity-container')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
