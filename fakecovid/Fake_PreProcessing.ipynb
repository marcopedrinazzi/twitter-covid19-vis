{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dirty-coffee",
   "metadata": {},
   "source": [
    "# Pre-Processing on the Dataset about Covid-19 Misinformation on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-disaster",
   "metadata": {},
   "source": [
    "The dataset had some issues, so after the hydratation, we've found some tweets which weren't about Covid-19.\n",
    "\n",
    "We've removed the following tweets:\n",
    "\n",
    "\n",
    "tweet_ID | year\n",
    "---|-----\n",
    "1027222566581231617 | 2018 \n",
    "1179751464215949313 | 2019 \n",
    "539901552937664513 | 2014 \n",
    "539914412401123328 | 2014\n",
    "1025660196218183680 | 2018 \n",
    "1088739576687091712 | 2019 \n",
    "1088800362847526913 | 2019 \n",
    "1101094162865242113 | 2019 \n",
    "1156661710247403523 | 2019\n",
    "1169318930156007425 | 2019\n",
    "1171793133690150919 | 2019\n",
    "1185034497521340416 | 2019\n",
    "1201776606706118656 | 2019\n",
    "900683060915523584 | 2017\n",
    "1015698605720686593 | 2018\n",
    "1126068077999853568 | 2019\n",
    "1165696327558344704 | 2019\n",
    "1181970325757517825 | 2019\n",
    "1184483676924436480 | 2019\n",
    "1188412233270714368 | 2019\n",
    "1200150134203568128 | 2019\n",
    "725937576532578304 | 2016\n",
    "900685413416751104 | 2017\n",
    "1132002388376788995 | 2019\n",
    "1134145939151740929 | 2019\n",
    "1167029938635100161 | 2019\n",
    "720102224605638656 | 2016\n",
    "903686853508857859 | 2017\n",
    "962814691415347200 | 2018\n",
    "995638432318738432 | 2018\n",
    "1014931242842775558 | 2018\n",
    "1023850585949274112 | 2018\n",
    "1023860881929711616 | 2018\n",
    "1053169444690763776 | 2018\n",
    "1105241563620360193 | 2019\n",
    "1117789925527773184 | 2019\n",
    "1153706403565117440 | 2019\n",
    "1153708733882601472 | 2019\n",
    "673699231098527744 | 2015\n",
    "827283890146463750 | 2017\n",
    "940623615615266822 | 2017\n",
    "971254280601694209 | 2018\n",
    "1105214769563402241 | 2019\n",
    "1105393223462207488 | 2019\n",
    "1105668048416108544 | 2019\n",
    "1172772374074118145 | 2019\n",
    "1172789409478922241 | 2019\n",
    "1176858065242660865 | 2019\n",
    "1209402080273809408 | 2019\n",
    "481020774195552256 | 2014\n",
    "899910766064750592 | 2017\n",
    "1097754735656951808 | 2019\n",
    "1105282184414416898 | 2019\n",
    "1150852667301912578 | 2019\n",
    "1068140903917858816 | 2018\n",
    "1184254666315456512 | 2019\n",
    "587827070077571072 | 2015\n",
    "1018297937720414208 | 2018\n",
    "1043078112010031104 | 2018\n",
    "957147456596213760 | 2018\n",
    "999352051829297153 | 2018\n",
    "1052255689563721729 | 2018\n",
    "1068212767897858049 | 2018\n",
    "1068277017999826944 | 2018\n",
    "1177110421758795777 | 2019\n",
    "1197668564137906176 | 2019\n",
    "1154784135447220224 | 2019\n",
    "515407029498679298 | 2014\n",
    "551629973195218946 | 2015\n",
    "580828044027424768 | 2015\n",
    "593080368430911488 | 2015\n",
    "934640524832645120 | 2017\n",
    "963112422822285312 | 2018\n",
    "1075785713914929152 | 2018\n",
    "1186606201259343873 | 2019\n",
    "615979734988681216 | 2015\n",
    "719965429175820288 | 2016\n",
    "933795562947796992 | 2017\n",
    "934535302642679808 | 2017\n",
    "997447899465297923 | 2018\n",
    "1167826809355792390 | 2019\n",
    "1184857295676674048 | 2019\n",
    "1206404759575506944 | 2019\n",
    "563148469796216833 | 2015\n",
    "951117592843956224 | 2018\n",
    "1200463040224718848 | 2019\n",
    "513491003462795264 | 2014\n",
    "1006592956059484160 | 2018\n",
    "1078854747086450689 | 2018\n",
    "1207528337180020736 | 2019\n",
    "1003387827395186688 | 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-october",
   "metadata": {},
   "source": [
    "### cleaner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-enzyme",
   "metadata": {},
   "source": [
    "During the hydratation process some Tweet IDs couldn't be found but for classification purposes we needed to have a .csv file matching the ids inside the .json file, so we've wrote this script to clean again the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "light-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "csv_dataframe = pd.read_csv('dataset/fakecovid_dataset_clean.csv',sep=\";\")\n",
    "csv_dataframe['tweet_id'] = csv_dataframe['tweet_id'].astype(str)\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_originale.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-meter",
   "metadata": {},
   "source": [
    "We've filtered the IDs as the dataset contains Tweets that are no longer available (about 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spatial-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_id = 0\n",
    "id_list = []\n",
    "for element in data:\n",
    "    id_list.append(data[index_id]['id_str'])\n",
    "    index_id=index_id+1\n",
    "\n",
    "boolean_series = csv_dataframe.tweet_id.isin(id_list)\n",
    "filtered_df = csv_dataframe[boolean_series]\n",
    "filtered_df.to_csv('fakecovid_filtered_dataset_clean.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-housing",
   "metadata": {},
   "source": [
    "### traduttore.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-vaccine",
   "metadata": {},
   "source": [
    "The .json file produced had multiple languages inside the text fields, we wrote this script to translate the fields which weren't in english (\"hashtags\" and \"full_text\") to english.\n",
    "\n",
    "Everything is done through the Google Translate APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-transcription",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 indice\n",
      "sleep dopo hashtag\n",
      "1 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "2 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "3 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "4 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "5 indice\n",
      "sleep dopo hashtag\n",
      "6 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "7 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "8 indice\n",
      "sleep dopo hashtag\n",
      "9 indice\n",
      "sleep dopo hashtag\n",
      "10 indice\n",
      "11 indice\n",
      "sleep dopo hashtag\n",
      "12 indice\n",
      "13 indice\n",
      "sleep dopo hashtag\n",
      "14 indice\n",
      "sleep dopo hashtag\n",
      "15 indice\n",
      "16 indice\n",
      "17 indice\n",
      "18 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "19 indice\n",
      "20 indice\n",
      "sleep dopo hashtag\n",
      "21 indice\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n",
      "sleep dopo hashtag\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#import sys\n",
    "#import string\n",
    "from google_trans_new import google_translator  \n",
    "import time\n",
    "\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_originale.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "index=0\n",
    "#translator_altro = Translator()\n",
    "translator = google_translator()  \n",
    "for element in data:\n",
    "    translated_full = translator.translate(data[index]['full_text'],lang_tgt='en')\n",
    "    data[index]['full_text']=translated_full\n",
    "    time.sleep(1)\n",
    "    print(str(index)+\" indice\")\n",
    "    for entity in data[index]['entities']['hashtags']:\n",
    "        translated = translator.translate(entity['text'],lang_tgt='en')#lang_tgt Ã¨ l'alt\n",
    "        entity['text']=translated\n",
    "        time.sleep(1)\n",
    "        print(\"sleep dopo hashtag\")\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "with open('fakecovid_result_translated_full.json', 'a') as f_w:\n",
    "    for line_w in data:\n",
    "        print(line)\n",
    "        json.dump(line_w, f_w)\n",
    "        f_w.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
