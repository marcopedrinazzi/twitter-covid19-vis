{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dirty-coffee",
   "metadata": {},
   "source": [
    "# Pre-Processing on the Dataset about Covid-19 Misinformation on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-disaster",
   "metadata": {},
   "source": [
    "The dataset had some issues, so after the hydratation, we've found some tweets which weren't about Covid-19.\n",
    "\n",
    "We've removed the following tweets:\n",
    "\n",
    "\n",
    "tweet_ID | year\n",
    "---|-----\n",
    "1027222566581231617 | 2018 \n",
    "1179751464215949313 | 2019 \n",
    "539901552937664513 | 2014 \n",
    "539914412401123328 | 2014\n",
    "1025660196218183680 | 2018 \n",
    "1088739576687091712 | 2019 \n",
    "1088800362847526913 | 2019 \n",
    "1101094162865242113 | 2019 \n",
    "1156661710247403523 | 2019\n",
    "1169318930156007425 | 2019\n",
    "1171793133690150919 | 2019\n",
    "1185034497521340416 | 2019\n",
    "1201776606706118656 | 2019\n",
    "900683060915523584 | 2017\n",
    "1015698605720686593 | 2018\n",
    "1126068077999853568 | 2019\n",
    "1165696327558344704 | 2019\n",
    "1181970325757517825 | 2019\n",
    "1184483676924436480 | 2019\n",
    "1188412233270714368 | 2019\n",
    "1200150134203568128 | 2019\n",
    "725937576532578304 | 2016\n",
    "900685413416751104 | 2017\n",
    "1132002388376788995 | 2019\n",
    "1134145939151740929 | 2019\n",
    "1167029938635100161 | 2019\n",
    "720102224605638656 | 2016\n",
    "903686853508857859 | 2017\n",
    "962814691415347200 | 2018\n",
    "995638432318738432 | 2018\n",
    "1014931242842775558 | 2018\n",
    "1023850585949274112 | 2018\n",
    "1023860881929711616 | 2018\n",
    "1053169444690763776 | 2018\n",
    "1105241563620360193 | 2019\n",
    "1117789925527773184 | 2019\n",
    "1153706403565117440 | 2019\n",
    "1153708733882601472 | 2019\n",
    "673699231098527744 | 2015\n",
    "827283890146463750 | 2017\n",
    "940623615615266822 | 2017\n",
    "971254280601694209 | 2018\n",
    "1105214769563402241 | 2019\n",
    "1105393223462207488 | 2019\n",
    "1105668048416108544 | 2019\n",
    "1172772374074118145 | 2019\n",
    "1172789409478922241 | 2019\n",
    "1176858065242660865 | 2019\n",
    "1209402080273809408 | 2019\n",
    "481020774195552256 | 2014\n",
    "899910766064750592 | 2017\n",
    "1097754735656951808 | 2019\n",
    "1105282184414416898 | 2019\n",
    "1150852667301912578 | 2019\n",
    "1068140903917858816 | 2018\n",
    "1184254666315456512 | 2019\n",
    "587827070077571072 | 2015\n",
    "1018297937720414208 | 2018\n",
    "1043078112010031104 | 2018\n",
    "957147456596213760 | 2018\n",
    "999352051829297153 | 2018\n",
    "1052255689563721729 | 2018\n",
    "1068212767897858049 | 2018\n",
    "1068277017999826944 | 2018\n",
    "1177110421758795777 | 2019\n",
    "1197668564137906176 | 2019\n",
    "1154784135447220224 | 2019\n",
    "515407029498679298 | 2014\n",
    "551629973195218946 | 2015\n",
    "580828044027424768 | 2015\n",
    "593080368430911488 | 2015\n",
    "934640524832645120 | 2017\n",
    "963112422822285312 | 2018\n",
    "1075785713914929152 | 2018\n",
    "1186606201259343873 | 2019\n",
    "615979734988681216 | 2015\n",
    "719965429175820288 | 2016\n",
    "933795562947796992 | 2017\n",
    "934535302642679808 | 2017\n",
    "997447899465297923 | 2018\n",
    "1167826809355792390 | 2019\n",
    "1184857295676674048 | 2019\n",
    "1206404759575506944 | 2019\n",
    "563148469796216833 | 2015\n",
    "951117592843956224 | 2018\n",
    "1200463040224718848 | 2019\n",
    "513491003462795264 | 2014\n",
    "1006592956059484160 | 2018\n",
    "1078854747086450689 | 2018\n",
    "1207528337180020736 | 2019\n",
    "1003387827395186688 | 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-foster",
   "metadata": {},
   "source": [
    "The dataset has a lot of entries which aren't in English. We have translated them to english (see below). Now we're going to visualize the different languages inside the dataset. The language classification is done by a Twitter ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accepted-technology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-490f65196ce640a09b109be93bb2c0bc\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-490f65196ce640a09b109be93bb2c0bc\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-490f65196ce640a09b109be93bb2c0bc\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 13, \"titleFontSize\": 15, \"titlePadding\": 15}, \"legend\": {\"labelFontSize\": 12, \"titleFontSize\": 14, \"titlePadding\": 10}, \"title\": {\"fontSize\": 17, \"offset\": 25}}, \"layer\": [{\"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"lang\", \"legend\": {\"columns\": 4, \"symbolLimit\": 0}, \"scale\": {\"domain\": [\"\\ud83c\\udde6\\ud83c\\uddf2 am\", \"\\ud83c\\udde6\\ud83c\\uddf7 ar\", \"\\ud83c\\udde7\\ud83c\\uddec bg\", \"\\ud83c\\udde7\\ud83c\\uddf3 bn\", \"\\ud83c\\udde8\\ud83c\\udde6 ca\", \"\\ud83c\\udde8\\ud83c\\uddff cs\", \"\\ud83c\\udde9\\ud83c\\uddf0 da\", \"\\ud83c\\uddee\\ud83c\\uddf9 it\", \"\\ud83c\\uddf9\\ud83c\\udde6 ta\", \"\\ud83c\\uddea\\ud83c\\uddf8 es\", \"\\ud83c\\uddea\\ud83c\\uddf9 et\", \"\\ud83c\\uddee\\ud83c\\uddf7 fa\", \"\\ud83c\\uddeb\\ud83c\\uddee fi\", \"\\ud83c\\uddeb\\ud83c\\uddf7 fr\", \"\\ud83c\\uddee\\ud83c\\uddf3 hi\", \"\\ud83c\\uddee\\ud83c\\uddf3 in\", \"\\ud83c\\uddf8\\ud83c\\uddf7 sr\", \"\\ud83c\\uddef\\ud83c\\uddf5 ja\", \"\\ud83c\\uddf1\\ud83c\\uddf9 lt\", \"\\ud83c\\uddf2\\ud83c\\uddf7 mr\", \"\\ud83c\\uddf3\\ud83c\\uddf1 nl\", \"\\ud83c\\uddf5\\ud83c\\uddf1 pl\", \"\\ud83c\\uddf5\\ud83c\\uddf9 pt\", \"\\ud83c\\uddf7\\ud83c\\uddfa ru\", \"\\ud83c\\uddec\\ud83c\\udde7 \\ud83c\\uddfa\\ud83c\\uddf8 en\", \"\\ud83c\\uddf9\\ud83c\\udded th\", \"\\ud83c\\uddf9\\ud83c\\uddf1 tl\", \"\\ud83c\\uddf9\\ud83c\\uddf7 tr\", \"\\ud83c\\uddf5\\ud83c\\uddf0 ur\", \"\\ud83c\\udde8\\ud83c\\uddf3 zh\", \"\\ud83c\\udde9\\ud83c\\uddea de\", \"\\ud83c\\uddee\\ud83c\\uddf1 iw\", \"\\ud83c\\udff4\\udb40\\udc75\\udb40\\udc6e\\udb40\\udc64\\udb40\\udc7f und\"], \"range\": [\"lime\", \"crimson\", \"lightcoral\", \"fuchsia\", \"#3C1414\", \"yellowgreen\", \"darkblue\", \"darkviolet\", \"darkgreen\", \"olive\", \"#49796B\", \" #00C000\", \"deeppink\", \"plum\", \"darkkhaki\", \"#00F0A8\", \"#44AA99\", \"lightseagreen\", \"#6495ED\", \"orangered\", \"coral\", \"turquoise\", \"blue\", \"black\", \"#843F5B\", \"tomato\", \"#B06500\", \"cyan\", \"chocolate\", \"#E23D28\", \"#D55E00\", \"sienna\", \"gold\"]}, \"title\": \"Languages\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"count\"}, \"y\": {\"type\": \"nominal\", \"field\": \"lang\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"dx\": 3}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"lang\", \"legend\": {\"columns\": 4, \"symbolLimit\": 0}, \"scale\": {\"domain\": [\"\\ud83c\\udde6\\ud83c\\uddf2 am\", \"\\ud83c\\udde6\\ud83c\\uddf7 ar\", \"\\ud83c\\udde7\\ud83c\\uddec bg\", \"\\ud83c\\udde7\\ud83c\\uddf3 bn\", \"\\ud83c\\udde8\\ud83c\\udde6 ca\", \"\\ud83c\\udde8\\ud83c\\uddff cs\", \"\\ud83c\\udde9\\ud83c\\uddf0 da\", \"\\ud83c\\uddee\\ud83c\\uddf9 it\", \"\\ud83c\\uddf9\\ud83c\\udde6 ta\", \"\\ud83c\\uddea\\ud83c\\uddf8 es\", \"\\ud83c\\uddea\\ud83c\\uddf9 et\", \"\\ud83c\\uddee\\ud83c\\uddf7 fa\", \"\\ud83c\\uddeb\\ud83c\\uddee fi\", \"\\ud83c\\uddeb\\ud83c\\uddf7 fr\", \"\\ud83c\\uddee\\ud83c\\uddf3 hi\", \"\\ud83c\\uddee\\ud83c\\uddf3 in\", \"\\ud83c\\uddf8\\ud83c\\uddf7 sr\", \"\\ud83c\\uddef\\ud83c\\uddf5 ja\", \"\\ud83c\\uddf1\\ud83c\\uddf9 lt\", \"\\ud83c\\uddf2\\ud83c\\uddf7 mr\", \"\\ud83c\\uddf3\\ud83c\\uddf1 nl\", \"\\ud83c\\uddf5\\ud83c\\uddf1 pl\", \"\\ud83c\\uddf5\\ud83c\\uddf9 pt\", \"\\ud83c\\uddf7\\ud83c\\uddfa ru\", \"\\ud83c\\uddec\\ud83c\\udde7 \\ud83c\\uddfa\\ud83c\\uddf8 en\", \"\\ud83c\\uddf9\\ud83c\\udded th\", \"\\ud83c\\uddf9\\ud83c\\uddf1 tl\", \"\\ud83c\\uddf9\\ud83c\\uddf7 tr\", \"\\ud83c\\uddf5\\ud83c\\uddf0 ur\", \"\\ud83c\\udde8\\ud83c\\uddf3 zh\", \"\\ud83c\\udde9\\ud83c\\uddea de\", \"\\ud83c\\uddee\\ud83c\\uddf1 iw\", \"\\ud83c\\udff4\\udb40\\udc75\\udb40\\udc6e\\udb40\\udc64\\udb40\\udc7f und\"], \"range\": [\"lime\", \"crimson\", \"lightcoral\", \"fuchsia\", \"#3C1414\", \"yellowgreen\", \"darkblue\", \"darkviolet\", \"darkgreen\", \"olive\", \"#49796B\", \" #00C000\", \"deeppink\", \"plum\", \"darkkhaki\", \"#00F0A8\", \"#44AA99\", \"lightseagreen\", \"#6495ED\", \"orangered\", \"coral\", \"turquoise\", \"blue\", \"black\", \"#843F5B\", \"tomato\", \"#B06500\", \"cyan\", \"chocolate\", \"#E23D28\", \"#D55E00\", \"sienna\", \"gold\"]}, \"title\": \"Languages\"}, \"text\": {\"type\": \"quantitative\", \"field\": \"count\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"count\"}, \"y\": {\"type\": \"nominal\", \"field\": \"lang\"}}}], \"data\": {\"name\": \"data-fc9449704ab1075cb4c0c7166231a081\"}, \"height\": 900, \"title\": \"Languages in the dataset\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-fc9449704ab1075cb4c0c7166231a081\": [{\"lang\": \"\\ud83c\\uddec\\ud83c\\udde7 \\ud83c\\uddfa\\ud83c\\uddf8 en\", \"count\": 752}, {\"lang\": \"\\ud83c\\udde6\\ud83c\\uddf7 ar\", \"count\": 42}, {\"lang\": \"\\ud83c\\uddeb\\ud83c\\uddf7 fr\", \"count\": 74}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf3 in\", \"count\": 17}, {\"lang\": \"\\ud83c\\uddea\\ud83c\\uddf8 es\", \"count\": 325}, {\"lang\": \"\\ud83c\\uddf5\\ud83c\\uddf9 pt\", \"count\": 64}, {\"lang\": \"\\ud83c\\uddf2\\ud83c\\uddf7 mr\", \"count\": 1}, {\"lang\": \"\\ud83c\\udff4\\udb40\\udc75\\udb40\\udc6e\\udb40\\udc64\\udb40\\udc7f und\", \"count\": 38}, {\"lang\": \"\\ud83c\\udde7\\ud83c\\uddf3 bn\", \"count\": 3}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf9 it\", \"count\": 23}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\uddf1 tl\", \"count\": 5}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf3 hi\", \"count\": 38}, {\"lang\": \"\\ud83c\\uddef\\ud83c\\uddf5 ja\", \"count\": 14}, {\"lang\": \"\\ud83c\\udde8\\ud83c\\uddf3 zh\", \"count\": 10}, {\"lang\": \"\\ud83c\\uddf3\\ud83c\\uddf1 nl\", \"count\": 6}, {\"lang\": \"\\ud83c\\udde6\\ud83c\\uddf2 am\", \"count\": 1}, {\"lang\": \"\\ud83c\\udde9\\ud83c\\uddea de\", \"count\": 7}, {\"lang\": \"\\ud83c\\uddea\\ud83c\\uddf9 et\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\uddf7 tr\", \"count\": 4}, {\"lang\": \"\\ud83c\\uddf8\\ud83c\\uddf7 sr\", \"count\": 3}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\udded th\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf1 iw\", \"count\": 2}, {\"lang\": \"\\ud83c\\uddf5\\ud83c\\uddf0 ur\", \"count\": 3}, {\"lang\": \"\\ud83c\\udde7\\ud83c\\uddec bg\", \"count\": 1}, {\"lang\": \"\\ud83c\\udde8\\ud83c\\udde6 ca\", \"count\": 2}, {\"lang\": \"\\ud83c\\uddf7\\ud83c\\uddfa ru\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddf1\\ud83c\\uddf9 lt\", \"count\": 3}, {\"lang\": \"\\ud83c\\uddf5\\ud83c\\uddf1 pl\", \"count\": 5}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\udde6 ta\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf7 fa\", \"count\": 4}, {\"lang\": \"\\ud83c\\uddeb\\ud83c\\uddee fi\", \"count\": 1}, {\"lang\": \"\\ud83c\\udde8\\ud83c\\uddff cs\", \"count\": 1}, {\"lang\": \"\\ud83c\\udde9\\ud83c\\uddf0 da\", \"count\": 1}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import flag\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "index_lang=0\n",
    "langs = []\n",
    "for element in data:\n",
    "    t=data[index_lang]['lang']\n",
    "    \n",
    "    if t == \"cs\":\n",
    "        langs.append(flag.flag(\"cz\")+\" \"+t)\n",
    "    elif t==\"da\":\n",
    "        langs.append(flag.flag(\"dk\")+\" \"+t)\n",
    "    elif t == \"en\":\n",
    "        langs.append(flag.flag(\"gb\")+\" \"+flag.flag(\"us\")+\" \"+t)\n",
    "    elif t == \"fa\":\n",
    "        langs.append(flag.flag(\"ir\")+\" \"+t)\n",
    "    elif t == \"hi\":\n",
    "        langs.append(flag.flag(\"in\")+\" \"+t)\n",
    "    elif t == \"iw\":\n",
    "        langs.append(flag.flag(\"il\")+\" \"+t)\n",
    "    elif t == \"ja\":\n",
    "        langs.append(flag.flag(\"jp\")+\" \"+t)\n",
    "    elif t == \"ur\":\n",
    "        langs.append(flag.flag(\"pk\")+\" \"+t)\n",
    "    elif t == \"zh\":\n",
    "        langs.append(flag.flag(\"cn\")+\" \"+t)\n",
    "    else:\n",
    "        langs.append(flag.flag(t)+\" \"+t)\n",
    "        \n",
    "    index_lang=index_lang+1\n",
    "    \n",
    "count=Counter(langs)\n",
    "df = pd.DataFrame.from_dict(count, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'lang', 0:'count'})\n",
    "\n",
    "\n",
    "domain = [flag.flag('am')+\" \"+'am',flag.flag('ar')+\" \"+'ar', flag.flag('bg')+\" \"+'bg', flag.flag('bn')+\" \"+'bn',flag.flag('ca')+\" \"+'ca',flag.flag('cz')+\" \"+'cs',flag.flag('dk')+\" \"+'da',flag.flag('it')+\" \"+'it',flag.flag('ta')+\" \"+'ta',flag.flag('es')+\" \"+'es',flag.flag('et')+\" \"+'et',flag.flag('ir')+\" \"+'fa',flag.flag('fi')+\" \"+'fi',flag.flag('fr')+\" \"+'fr',flag.flag('in')+\" \"+'hi',flag.flag('in')+\" \"+'in',flag.flag('sr')+\" \"+'sr',flag.flag('jp')+\" \"+'ja',flag.flag('lt')+\" \"+'lt',flag.flag('mr')+\" \"+'mr',flag.flag('nl')+\" \"+'nl',flag.flag('pl')+\" \"+'pl',flag.flag('pt')+\" \"+'pt',flag.flag('ru')+\" \"+'ru',flag.flag(\"gb\")+\" \"+flag.flag(\"us\")+\" \"+'en',flag.flag('th')+\" \"+'th',flag.flag('tl')+\" \"+'tl',flag.flag('tr')+\" \"+'tr', flag.flag('pk')+\" \"+'ur',flag.flag('cn')+\" \"+'zh',flag.flag('de')+\" \"+'de',flag.flag('il')+\" \"+'iw',flag.flag('und')+\" \"+'und']\n",
    "#range_ = ['red','green','blue','mediumvioletred','purple','black','salmon','brown','darkgrey','orange','fuchsia','teal','coral','cyan','violet','crimson','lime','slateblue','slategray','darkkhaki','tan','indigo','olive','gold','maroon','darkslategray','indianred','tomato','turquoise','darkgreen','chocolate','plum','peru']\n",
    "range_ = ['lime', 'crimson', 'lightcoral', 'fuchsia', '#3C1414', 'yellowgreen', 'darkblue', 'darkviolet', 'darkgreen', 'olive', '#49796B', ' #00C000', 'deeppink', 'plum', 'darkkhaki', '#00F0A8', '#44AA99', 'lightseagreen', '#6495ED', 'orangered', 'coral', 'turquoise', 'blue', 'black', '#843F5B', 'tomato', '#B06500', 'cyan', 'chocolate', '#E23D28', '#D55E00', 'sienna', 'gold']\n",
    "#domain = [flag.flag('ar')+\" \"+'ar',flag.flag('bg')+\" \"+'bg',flag.flag('ca')+\" \"+'ca',flag.flag('cz')+\" \"+'cs',flag.flag('gb')+\" \"+'cy',flag.flag('dk')+\" \"+'da',flag.flag('de')+\" \"+'de',flag.flag('gr')+\" \"+'el',flag.flag('gb')+\" \"+flag.flag('us')+\" \"+'en',flag.flag('es')+\" \"+'es',flag.flag('et')+\" \"+'et',flag.flag('eu')+\" \"+'eu',flag.flag('ir')+\" \"+'fa',flag.flag('fi')+\" \"+'fi',flag.flag('fr')+\" \"+'fr',flag.flag('in')+\" \"+'hi',flag.flag('ht')+\" \"+'ht',flag.flag('hu')+\" \"+'hu',flag.flag('in')+\" \"+'in',flag.flag('is')+\" \"+'is',flag.flag('it')+\" \"+'it',flag.flag('il')+\" \"+'iw',flag.flag('jp')+\" \"+'ja',flag.flag('km')+\" \"+'km',flag.flag('kn')+\" \"+'kn',flag.flag('kr')+\" \"+'ko',flag.flag('lt')+\" \"+'lt',flag.flag('lv')+\" \"+'lv',flag.flag('mr')+\" \"+'mr',flag.flag('my')+\" \"+'my',flag.flag('ne')+\" \"+'ne',flag.flag('nl')+\" \"+'nl',flag.flag('no')+\" \"+'no',flag.flag('pl')+\" \"+'pl',flag.flag('pt')+\" \"+'pt',flag.flag('ro')+\" \"+'ro',flag.flag('ru')+\" \"+'ru',flag.flag('sd')+\" \"+'sd',flag.flag('sl')+\" \"+'sl',flag.flag('sr')+\" \"+'sr',flag.flag('sv')+\" \"+'sv',flag.flag('ta')+\" \"+'ta',flag.flag('in')+\" \"+'te',flag.flag('th')+\" \"+'th',flag.flag('tl')+\" \"+'tl',flag.flag('tr')+\" \"+'tr',flag.flag('ua')+\" \"+'uk',flag.flag('und')+\" \"+'und',flag.flag('pk')+\" \"+'ur',flag.flag('vi')+\" \"+'vi',flag.flag('cn')+\" \"+'zh']\n",
    "#range_ = ['darkolivegreen', '#577120', '#985629', '#006090', 'salmon', '#DDE26A', 'mediumvioletred', 'limegreen', '#FF6FFF', '#FF007F', 'darkslategray', 'violet', '#18A8D8', '#18A8D8', 'purple', 'green', '#8A3324', 'indigo',\n",
    "\n",
    "chart = alt.Chart(df).mark_bar().encode(\n",
    "    x='count:Q',\n",
    "    y='lang:N',\n",
    "    color=alt.Color('lang', scale=alt.Scale(domain=domain, range=range_),  legend=alt.Legend(columns=4, symbolLimit=0),title=\"Languages\")\n",
    ")\n",
    "\n",
    "text = chart.mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n",
    ").encode(\n",
    "    text='count:Q'\n",
    ")\n",
    "\n",
    "\n",
    "(chart + text).properties(title=\"Languages in the dataset\",height=900).configure_title(\n",
    "    fontSize=17,\n",
    "    offset=25\n",
    ").configure_axis(\n",
    "    labelFontSize=13,\n",
    "    titleFontSize=15,\n",
    "    titlePadding=15\n",
    ").configure_legend(\n",
    "    titleFontSize=14,\n",
    "    labelFontSize=12,\n",
    "    titlePadding=10\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-october",
   "metadata": {},
   "source": [
    "### cleaner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-enzyme",
   "metadata": {},
   "source": [
    "During the hydratation process some Tweet IDs couldn't be found but for classification purposes we needed to have a .csv file matching the ids inside the .json file, so we've wrote this script to clean again the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "csv_dataframe = pd.read_csv('dataset/fakecovid_filtered_dataset_clean_final.csv',sep=\";\")\n",
    "csv_dataframe['tweet_id'] = csv_dataframe['tweet_id'].astype(str)\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-meter",
   "metadata": {},
   "source": [
    "We've filtered the IDs as the dataset contains Tweets that are no longer available (about 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_id = 0\n",
    "id_list = []\n",
    "for element in data:\n",
    "    id_list.append(data[index_id]['id_str'])\n",
    "    index_id=index_id+1\n",
    "\n",
    "boolean_series = csv_dataframe.tweet_id.isin(id_list)\n",
    "filtered_df = csv_dataframe[boolean_series]\n",
    "filtered_df.to_csv('FINAL_fakecovid_final_filtered_dataset_clean.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-housing",
   "metadata": {},
   "source": [
    "### traduttore.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-vaccine",
   "metadata": {},
   "source": [
    "The .json file produced had multiple languages inside the text fields, we wrote this script to translate the fields which weren't in english (\"hashtags\" and \"full_text\") to english.\n",
    "\n",
    "Everything is done through the Google Translate APIs.\n",
    "\n",
    "Due to Google Translate Limitations to a massive number of requests, the for loop below does a pre-filtering, based on the lang filed from the .json file. The lang field is filled automatically during the hydratation process, the language classification is done by machine learning algorithms.\n",
    "\n",
    "The script below is the full version, we've dived the execution in two phases: the first one worked on the \"full_text\" field, the second one worked on the \"hashtag\" filed considering that the \"full_text\" filed was already OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import string\n",
    "from google_trans_new import google_translator  \n",
    "import time\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "index=0\n",
    "translator = google_translator()  \n",
    "for element in data:\n",
    "    if data[index]['lang']==\"en\":\n",
    "        print(str(index)+\" già inglese\")\n",
    "    else:\n",
    "        translated  = translator.translate(data[index]['full_text'],lang_tgt='en')  \n",
    "        data[index]['full_text'] = translated\n",
    "        time.sleep(1) #sleep to avoid being blocked by Google \n",
    "        #print(str(index)+\" indice\" + data[index]['full_text'])\n",
    "        for entity in data[index]['entities']['hashtags']:\n",
    "            translated = translator.translate(entity['text'],lang_tgt='en')#lang_tgt è l'alt\n",
    "            entity['text']=translated\n",
    "            time.sleep(1)  #sleep to avoid being blocked by Google\n",
    "            #print(str(index)+\" indice\" + entity['text'])\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "with open('fakecovid_result_final_translated_full.json', 'a') as f_w:\n",
    "    for line_w in data:\n",
    "        #print(\"sto stampando\")\n",
    "        json.dump(line_w, f_w)\n",
    "        f_w.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-demand",
   "metadata": {},
   "source": [
    "### URL pre processing to create a good visualisation of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-infrastructure",
   "metadata": {},
   "source": [
    "We have to extract the link from the Tweet, so with the BeautifulSoup module, we obtain the title of the page (which is located at the link indicated in the Tweet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import csv\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "\n",
    "csv_dataframe = pd.read_csv('dataset/FINAL_fakecovid_final_filtered_dataset_clean.csv',sep=\";\")\n",
    "csv_dataframe['tweet_id'] = csv_dataframe['tweet_id'].astype(str)\n",
    "csv_list = csv_dataframe.values.tolist()\n",
    "lista_unica_csv=list(itertools.chain.from_iterable(csv_list))\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "index=0\n",
    "urls = []\n",
    "titles = []\n",
    "dates = []\n",
    "category = []\n",
    "for element in data:\n",
    "    #print(index)\n",
    "    if data[index]['entities']['urls'] is not None:\n",
    "        for entity in data[index]['entities']['urls']:\n",
    "            if entity['expanded_url'].lower() not in urls:\n",
    "                token_id = data[index]['id_str']                          \n",
    "                indice_csv = lista_unica_csv.index(token_id)   \n",
    "                value_cat =  lista_unica_csv[indice_csv+1].lower()\n",
    "                if value_cat == \"false\":\n",
    "                    value_cat = \"fake\"\n",
    "                try:\n",
    "                    r = requests.get(entity['expanded_url'], timeout=10)\n",
    "                except requests.exceptions.Timeout:\n",
    "                    titles.append(\"[TIMEOUT ERROR]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                    urls.append(entity['expanded_url'].lower())\n",
    "                    category.append(value_cat.replace(\" \", \"\"))\n",
    "                    d = parse(data[index]['created_at'])\n",
    "                    d = d.strftime('%Y/%m/%d')\n",
    "                    dates.append(d)\n",
    "                except requests.ConnectionError:\n",
    "                    titles.append(\"[CONNECTION ERROR]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                    urls.append(entity['expanded_url'].lower())\n",
    "                    category.append(value_cat.replace(\" \", \"\"))\n",
    "                    d = parse(data[index]['created_at'])\n",
    "                    d = d.strftime('%Y/%m/%d')\n",
    "                    dates.append(d)\n",
    "                else:\n",
    "                    soup = BeautifulSoup(r.text,features=\"lxml\")\n",
    "                    if soup.title is None:\n",
    "                        titles.append(\"[NO TITLE ERROR]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                        urls.append(entity['expanded_url'].lower())\n",
    "                        category.append(value_cat.replace(\" \", \"\"))\n",
    "                        d = parse(data[index]['created_at'])\n",
    "                        d = d.strftime('%Y/%m/%d')\n",
    "                        dates.append(d)\n",
    "                    else:\n",
    "                        titles.append(\"[\"+soup.title.text+\"]\"+\"(\"+entity['expanded_url'].lower()+\")\")\n",
    "                        urls.append(entity['expanded_url'].lower())\n",
    "                        category.append(value_cat.replace(\" \", \"\"))\n",
    "                        d = parse(data[index]['created_at'])\n",
    "                        d = d.strftime('%Y/%m/%d')\n",
    "                        dates.append(d)\n",
    "            else:\n",
    "                print(\"URL già presente\")\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-beatles",
   "metadata": {},
   "source": [
    "We create the Pandas DataFrame, in order to create a table, containing all the links in the Tweets, that it will be transcripted in a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'Type': category,\n",
    "    'Link': titles,\n",
    "    'First-Shared': dates\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-stake",
   "metadata": {},
   "source": [
    "Then, we create the CSV file, that we'll use to create the Dash DataTable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('urls.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
