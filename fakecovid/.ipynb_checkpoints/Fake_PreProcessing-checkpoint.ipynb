{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dirty-coffee",
   "metadata": {},
   "source": [
    "# Pre-Processing on the Dataset about Covid-19 Misinformation on Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-disaster",
   "metadata": {},
   "source": [
    "The dataset had some issues, so after the hydratation, we've found some tweets which weren't about Covid-19.\n",
    "\n",
    "We've removed the following tweets:\n",
    "\n",
    "\n",
    "tweet_ID | year\n",
    "---|-----\n",
    "1027222566581231617 | 2018 \n",
    "1179751464215949313 | 2019 \n",
    "539901552937664513 | 2014 \n",
    "539914412401123328 | 2014\n",
    "1025660196218183680 | 2018 \n",
    "1088739576687091712 | 2019 \n",
    "1088800362847526913 | 2019 \n",
    "1101094162865242113 | 2019 \n",
    "1156661710247403523 | 2019\n",
    "1169318930156007425 | 2019\n",
    "1171793133690150919 | 2019\n",
    "1185034497521340416 | 2019\n",
    "1201776606706118656 | 2019\n",
    "900683060915523584 | 2017\n",
    "1015698605720686593 | 2018\n",
    "1126068077999853568 | 2019\n",
    "1165696327558344704 | 2019\n",
    "1181970325757517825 | 2019\n",
    "1184483676924436480 | 2019\n",
    "1188412233270714368 | 2019\n",
    "1200150134203568128 | 2019\n",
    "725937576532578304 | 2016\n",
    "900685413416751104 | 2017\n",
    "1132002388376788995 | 2019\n",
    "1134145939151740929 | 2019\n",
    "1167029938635100161 | 2019\n",
    "720102224605638656 | 2016\n",
    "903686853508857859 | 2017\n",
    "962814691415347200 | 2018\n",
    "995638432318738432 | 2018\n",
    "1014931242842775558 | 2018\n",
    "1023850585949274112 | 2018\n",
    "1023860881929711616 | 2018\n",
    "1053169444690763776 | 2018\n",
    "1105241563620360193 | 2019\n",
    "1117789925527773184 | 2019\n",
    "1153706403565117440 | 2019\n",
    "1153708733882601472 | 2019\n",
    "673699231098527744 | 2015\n",
    "827283890146463750 | 2017\n",
    "940623615615266822 | 2017\n",
    "971254280601694209 | 2018\n",
    "1105214769563402241 | 2019\n",
    "1105393223462207488 | 2019\n",
    "1105668048416108544 | 2019\n",
    "1172772374074118145 | 2019\n",
    "1172789409478922241 | 2019\n",
    "1176858065242660865 | 2019\n",
    "1209402080273809408 | 2019\n",
    "481020774195552256 | 2014\n",
    "899910766064750592 | 2017\n",
    "1097754735656951808 | 2019\n",
    "1105282184414416898 | 2019\n",
    "1150852667301912578 | 2019\n",
    "1068140903917858816 | 2018\n",
    "1184254666315456512 | 2019\n",
    "587827070077571072 | 2015\n",
    "1018297937720414208 | 2018\n",
    "1043078112010031104 | 2018\n",
    "957147456596213760 | 2018\n",
    "999352051829297153 | 2018\n",
    "1052255689563721729 | 2018\n",
    "1068212767897858049 | 2018\n",
    "1068277017999826944 | 2018\n",
    "1177110421758795777 | 2019\n",
    "1197668564137906176 | 2019\n",
    "1154784135447220224 | 2019\n",
    "515407029498679298 | 2014\n",
    "551629973195218946 | 2015\n",
    "580828044027424768 | 2015\n",
    "593080368430911488 | 2015\n",
    "934640524832645120 | 2017\n",
    "963112422822285312 | 2018\n",
    "1075785713914929152 | 2018\n",
    "1186606201259343873 | 2019\n",
    "615979734988681216 | 2015\n",
    "719965429175820288 | 2016\n",
    "933795562947796992 | 2017\n",
    "934535302642679808 | 2017\n",
    "997447899465297923 | 2018\n",
    "1167826809355792390 | 2019\n",
    "1184857295676674048 | 2019\n",
    "1206404759575506944 | 2019\n",
    "563148469796216833 | 2015\n",
    "951117592843956224 | 2018\n",
    "1200463040224718848 | 2019\n",
    "513491003462795264 | 2014\n",
    "1006592956059484160 | 2018\n",
    "1078854747086450689 | 2018\n",
    "1207528337180020736 | 2019\n",
    "1003387827395186688 | 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-foster",
   "metadata": {},
   "source": [
    "The dataset has a lot of entries which aren't in English. We have translated them to english (see below). Now we're going to visualize the different languages inside the dataset. The language classification is done by a Twitter ML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accepted-technology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-fbdae24a65224e978bb6a4fa9fffedab\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-fbdae24a65224e978bb6a4fa9fffedab\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-fbdae24a65224e978bb6a4fa9fffedab\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"bar\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"lang\", \"legend\": {\"columns\": 4, \"symbolLimit\": 0}, \"scale\": {\"domain\": [\"\\ud83c\\udde6\\ud83c\\uddf2 am\", \"\\ud83c\\udde6\\ud83c\\uddf7 ar\", \"\\ud83c\\udde7\\ud83c\\uddec bg\", \"bn\", \"ca\", \"cs\", \"da\", \"it\", \"ta\", \"es\", \"et\", \"fa\", \"fi\", \"fr\", \"hi\", \"in\", \"sr\", \"ja\", \"lt\", \"mr\", \"nl\", \"pl\", \"pt\", \"ru\", \"en\", \"th\", \"tl\", \"tr\", \"und\", \"ur\", \"zh\", \"de\", \"iw\"], \"range\": [\"red\", \"green\", \"blue\", \"pink\", \"purple\", \"black\", \"yellow\", \"brown\", \"darkgrey\", \"orange\", \"fuchsia\", \"teal\", \"coral\", \"cyan\", \"violet\", \"crimson\", \"lime\", \"lightblue\", \"lightgrey\", \"khaki\", \"tan\", \"indigo\", \"olive\", \"gold\", \"maroon\", \"silver\", \"azure\", \"tomato\", \"lightcyna\", \"darkgreen\", \"chocolate\", \"plum\", \"peru\"]}}, \"x\": {\"type\": \"quantitative\", \"field\": \"count\"}, \"y\": {\"type\": \"nominal\", \"field\": \"lang\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"baseline\": \"middle\", \"dx\": 3}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"lang\", \"legend\": {\"columns\": 4, \"symbolLimit\": 0}, \"scale\": {\"domain\": [\"\\ud83c\\udde6\\ud83c\\uddf2 am\", \"\\ud83c\\udde6\\ud83c\\uddf7 ar\", \"\\ud83c\\udde7\\ud83c\\uddec bg\", \"bn\", \"ca\", \"cs\", \"da\", \"it\", \"ta\", \"es\", \"et\", \"fa\", \"fi\", \"fr\", \"hi\", \"in\", \"sr\", \"ja\", \"lt\", \"mr\", \"nl\", \"pl\", \"pt\", \"ru\", \"en\", \"th\", \"tl\", \"tr\", \"und\", \"ur\", \"zh\", \"de\", \"iw\"], \"range\": [\"red\", \"green\", \"blue\", \"pink\", \"purple\", \"black\", \"yellow\", \"brown\", \"darkgrey\", \"orange\", \"fuchsia\", \"teal\", \"coral\", \"cyan\", \"violet\", \"crimson\", \"lime\", \"lightblue\", \"lightgrey\", \"khaki\", \"tan\", \"indigo\", \"olive\", \"gold\", \"maroon\", \"silver\", \"azure\", \"tomato\", \"lightcyna\", \"darkgreen\", \"chocolate\", \"plum\", \"peru\"]}}, \"text\": {\"type\": \"quantitative\", \"field\": \"count\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"count\"}, \"y\": {\"type\": \"nominal\", \"field\": \"lang\"}}}], \"data\": {\"name\": \"data-a1db34b673509d1223e6f9b1b02b143f\"}, \"height\": 900, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-a1db34b673509d1223e6f9b1b02b143f\": [{\"lang\": \"\\ud83c\\uddec\\ud83c\\udde7\\ud83c\\uddfa\\ud83c\\uddf8 en\", \"count\": 752}, {\"lang\": \"\\ud83c\\udde6\\ud83c\\uddf7 ar\", \"count\": 42}, {\"lang\": \"\\ud83c\\uddeb\\ud83c\\uddf7 fr\", \"count\": 74}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf3 in\", \"count\": 17}, {\"lang\": \"\\ud83c\\uddea\\ud83c\\uddf8 es\", \"count\": 325}, {\"lang\": \"\\ud83c\\uddf5\\ud83c\\uddf9 pt\", \"count\": 64}, {\"lang\": \"\\ud83c\\uddf2\\ud83c\\uddf7 mr\", \"count\": 1}, {\"lang\": \"\\ud83c\\udff4\\udb40\\udc75\\udb40\\udc6e\\udb40\\udc64\\udb40\\udc7f und\", \"count\": 38}, {\"lang\": \"\\ud83c\\udde7\\ud83c\\uddf3 bn\", \"count\": 3}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf9 it\", \"count\": 23}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\uddf1 tl\", \"count\": 5}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf3 hi\", \"count\": 38}, {\"lang\": \"\\ud83c\\uddef\\ud83c\\uddf5 ja\", \"count\": 14}, {\"lang\": \"\\ud83c\\udde8\\ud83c\\uddf3 zh\", \"count\": 10}, {\"lang\": \"\\ud83c\\uddf3\\ud83c\\uddf1 nl\", \"count\": 6}, {\"lang\": \"\\ud83c\\udde6\\ud83c\\uddf2 am\", \"count\": 1}, {\"lang\": \"\\ud83c\\udde9\\ud83c\\uddea de\", \"count\": 7}, {\"lang\": \"\\ud83c\\uddea\\ud83c\\uddf9 et\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\uddf7 tr\", \"count\": 4}, {\"lang\": \"\\ud83c\\uddf8\\ud83c\\uddf7 sr\", \"count\": 3}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\udded th\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf1 iw\", \"count\": 2}, {\"lang\": \"\\ud83c\\uddf5\\ud83c\\uddf0 ur\", \"count\": 3}, {\"lang\": \"\\ud83c\\udde7\\ud83c\\uddec bg\", \"count\": 1}, {\"lang\": \"\\ud83c\\udde8\\ud83c\\udde6 ca\", \"count\": 2}, {\"lang\": \"\\ud83c\\uddf7\\ud83c\\uddfa ru\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddf1\\ud83c\\uddf9 lt\", \"count\": 3}, {\"lang\": \"\\ud83c\\uddf5\\ud83c\\uddf1 pl\", \"count\": 5}, {\"lang\": \"\\ud83c\\uddf9\\ud83c\\udde6 ta\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddee\\ud83c\\uddf7 fa\", \"count\": 4}, {\"lang\": \"\\ud83c\\uddeb\\ud83c\\uddee fi\", \"count\": 1}, {\"lang\": \"\\ud83c\\uddf7\\ud83c\\uddf8\\ud83c\\uddf2\\ud83c\\uddea cs\", \"count\": 1}, {\"lang\": \"\\ud83c\\udde9\\ud83c\\uddf0 da\", \"count\": 1}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import flag\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "index_lang=0\n",
    "langs = []\n",
    "for element in data:\n",
    "    t=data[index_lang]['lang']\n",
    "    if t == \"cs\":\n",
    "        langs.append(flag.flag(\"rs\")+flag.flag(\"me\")+\" \"+t)\n",
    "    elif t==\"da\":\n",
    "        langs.append(flag.flag(\"dk\")+\" \"+t)\n",
    "    elif t == \"en\":\n",
    "        langs.append(flag.flag(\"gb\")+flag.flag(\"us\")+\" \"+t)\n",
    "    elif t == \"fa\":\n",
    "        langs.append(flag.flag(\"ir\")+\" \"+t)\n",
    "    elif t == \"hi\":\n",
    "        langs.append(flag.flag(\"in\")+\" \"+t)\n",
    "    elif t == \"iw\":\n",
    "        langs.append(flag.flag(\"il\")+\" \"+t)\n",
    "    elif t == \"ja\":\n",
    "        langs.append(flag.flag(\"jp\")+\" \"+t)\n",
    "    elif t == \"ur\":\n",
    "        langs.append(flag.flag(\"pk\")+\" \"+t)\n",
    "    elif t == \"zh\":\n",
    "        langs.append(flag.flag(\"cn\")+\" \"+t)\n",
    "    else:\n",
    "        langs.append(flag.flag(t)+\" \"+t)\n",
    "        \n",
    "    index_lang=index_lang+1\n",
    "    \n",
    "count=Counter(langs)\n",
    "df = pd.DataFrame.from_dict(count, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'lang', 0:'count'})\n",
    "\n",
    "#TODO il mapping va finito con i flag, controlla se i flag che ho messo abbiano senso (nell'if). \n",
    "#Nel domain va rispettata la struttura dell'if sopra.\n",
    "#I colori vanno sistemati perchè quelli chiari si vedono male, ma i colori cmq devono essere diversi tra lor\n",
    "#per colorare le barre\n",
    "domain = [flag.flag('am')+\" \"+'am',flag.flag('ar')+\" \"+'ar', flag.flag('bg')+\" \"+'bg', flag.flag('bn')+\" \"+'bn','ca','cs','da','it','ta','es','et','fa','fi','fr','hi','in','sr','ja','lt','mr','nl','pl','pt','ru','en','th','tl','tr','und','ur','zh','de','iw']\n",
    "range_ = ['red','green','blue','pink','purple','black','yellow','brown','darkgrey','orange','fuchsia','teal','coral','cyan','violet','crimson','lime','lightblue','lightgrey','khaki','tan','indigo','olive','gold','maroon','silver','azure','tomato','lightcyna','darkgreen','chocolate','plum','peru']\n",
    "\n",
    "\n",
    "chart = alt.Chart(df).mark_bar().encode(\n",
    "    x='count:Q',\n",
    "    y='lang:N',\n",
    "    color=alt.Color('lang', scale=alt.Scale(domain=domain, range=range_),  legend=alt.Legend(columns=4, symbolLimit=0))\n",
    ")\n",
    "\n",
    "text = chart.mark_text(\n",
    "    align='left',\n",
    "    baseline='middle',\n",
    "    dx=3  # Nudges text to right so it doesn't appear on top of the bar\n",
    ").encode(\n",
    "    text='count:Q'\n",
    ")\n",
    "\n",
    "\n",
    "(chart + text).properties(height=900)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-october",
   "metadata": {},
   "source": [
    "### cleaner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-enzyme",
   "metadata": {},
   "source": [
    "During the hydratation process some Tweet IDs couldn't be found but for classification purposes we needed to have a .csv file matching the ids inside the .json file, so we've wrote this script to clean again the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import json\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "csv_dataframe = pd.read_csv('../dataset/fakecovid_filtered_dataset_clean_final.csv',sep=\";\")\n",
    "csv_dataframe['tweet_id'] = csv_dataframe['tweet_id'].astype(str)\n",
    "\n",
    "data = []\n",
    "with open('../dataset/fakecovid_result_final_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-meter",
   "metadata": {},
   "source": [
    "We've filtered the IDs as the dataset contains Tweets that are no longer available (about 100):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_id = 0\n",
    "id_list = []\n",
    "for element in data:\n",
    "    id_list.append(data[index_id]['id_str'])\n",
    "    index_id=index_id+1\n",
    "\n",
    "boolean_series = csv_dataframe.tweet_id.isin(id_list)\n",
    "filtered_df = csv_dataframe[boolean_series]\n",
    "filtered_df.to_csv('FINAL_fakecovid_final_filtered_dataset_clean.csv',sep=';',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-housing",
   "metadata": {},
   "source": [
    "### traduttore.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-vaccine",
   "metadata": {},
   "source": [
    "The .json file produced had multiple languages inside the text fields, we wrote this script to translate the fields which weren't in english (\"hashtags\" and \"full_text\") to english.\n",
    "\n",
    "Everything is done through the Google Translate APIs.\n",
    "\n",
    "Due to Google Translate Limitations to a massive number of requests, the for loop below does a pre-filtering, based on the lang filed from the .json file. The lang field is filled automatically during the hydratation process, the language classification is done by machine learning algorithms.\n",
    "\n",
    "The script below is the full version, we've dived the execution in two phases: the first one worked on the \"full_text\" field, the second one worked on the \"hashtag\" filed considering that the \"full_text\" filed was already OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import string\n",
    "from google_trans_new import google_translator  \n",
    "import time\n",
    "\n",
    "data = []\n",
    "with open('dataset/fakecovid_result_final.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "index=0\n",
    "translator = google_translator()  \n",
    "for element in data:\n",
    "    if data[index]['lang']==\"en\":\n",
    "        print(str(index)+\" già inglese\")\n",
    "    else:\n",
    "        translated  = translator.translate(data[index]['full_text'],lang_tgt='en')  \n",
    "        data[index]['full_text'] = translated\n",
    "        time.sleep(1) #sleep to avoid being blocked by Google \n",
    "        #print(str(index)+\" indice\" + data[index]['full_text'])\n",
    "        for entity in data[index]['entities']['hashtags']:\n",
    "            translated = translator.translate(entity['text'],lang_tgt='en')#lang_tgt è l'alt\n",
    "            entity['text']=translated\n",
    "            time.sleep(1)  #sleep to avoid being blocked by Google\n",
    "            #print(str(index)+\" indice\" + entity['text'])\n",
    "    index=index+1\n",
    "\n",
    "\n",
    "with open('fakecovid_result_final_translated_full.json', 'a') as f_w:\n",
    "    for line_w in data:\n",
    "        #print(\"sto stampando\")\n",
    "        json.dump(line_w, f_w)\n",
    "        f_w.write('\\n')\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
