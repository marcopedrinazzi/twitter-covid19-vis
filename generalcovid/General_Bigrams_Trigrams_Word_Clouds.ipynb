{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dependent-haiti",
   "metadata": {},
   "source": [
    "# Bigrams and Trigrams Word Clouds - General Covid-19 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-documentation",
   "metadata": {},
   "source": [
    "### trigrams_cloud.py and bigrams_cloud.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-input",
   "metadata": {},
   "source": [
    "In order to create the word clouds, we need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "synthetic-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee, islice \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "#import sys\n",
    "import string\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import emoji\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder,TrigramCollocationFinder, TrigramAssocMeasures\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "import num2words\n",
    "from PIL import Image\n",
    "import altair as alt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-granny",
   "metadata": {},
   "source": [
    "Then we have defined the following functions to clean the tweets' text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incident-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return(result)\n",
    "\n",
    "def remove_twitter_urls(text):\n",
    "    clean = re.sub(r\"pic.twitter\\S+\", \"\",text)\n",
    "    return(clean)\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "\n",
    "def noamp(text):\n",
    "    clean = re.sub(\"&amp\", \" \",text)\n",
    "    return (clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-riding",
   "metadata": {},
   "source": [
    "To read the JSON file that has all the tweets, it is necessary to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "upset-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset/general_result_translated_full.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-industry",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-direction",
   "metadata": {},
   "source": [
    "We're interested in the \"full_text\" field, that it has been cleaned with specific functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cosmetic-paint",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'contractions' has no attribute 'fix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-59deb885d5eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                              \u001b[1;31m# Put everything in lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m#data[index]['full_text'] = re.sub(\"\\'\\w+\", '', data[index]['full_text'])                # Remove everything after '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_urls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'full_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'contractions' has no attribute 'fix'"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "stop_words = stopwords.words('english')\n",
    "new_bigram=[]\n",
    "for element in data:\n",
    "    data[index]['full_text'] = data[index]['full_text'].lower()                              # Put everything in lowercase\n",
    "    data[index]['full_text'] = contractions.fix(data[index]['full_text'])\n",
    "    #data[index]['full_text'] = re.sub(\"\\'\\w+\", '', data[index]['full_text'])                # Remove everything after '\n",
    "    data[index]['full_text'] = remove_urls(data[index]['full_text'])\n",
    "    data[index]['full_text'] = remove_twitter_urls(data[index]['full_text'])\n",
    "    data[index]['full_text'] = remove_emoticons(data[index]['full_text'])\n",
    "    data[index]['full_text'] = remove_emoji(data[index]['full_text'])\n",
    "    data[index]['full_text'] = give_emoji_free_text(data[index]['full_text'])\n",
    "    data[index]['full_text'] = noamp(data[index]['full_text'])                               # No amp with space\n",
    "    data[index]['full_text'] = re.sub(\"#\\S+\", \" \",  data[index]['full_text'])                # Remove hashtags\n",
    "    data[index]['full_text'] = re.sub(\"@\\S+\", \" \",  data[index]['full_text'])                # No mentions\n",
    "    data[index]['full_text'] = data[index]['full_text'].translate(str.maketrans('', '', string.punctuation)) # No puntuaction\n",
    "    data[index]['full_text'] = data[index]['full_text'].encode('ascii', 'ignore').decode()   # No unicode\n",
    "    data[index]['full_text'] = re.sub(\"^rt \", \" \", data[index]['full_text'])                 # No RT\n",
    "    data[index]['full_text'] = re.sub('\\s{2,}', \" \", data[index]['full_text'])               # Remove big spaces\n",
    "    bigram_tokens=list(nltk.bigrams(nltk.word_tokenize(data[index]['full_text'])))\n",
    "    #print(bigram_tokens)\n",
    "    clean_bigram_tokens = [gram for gram in bigram_tokens if not any(stop in gram for stop in stop_words)]\n",
    "    new_bigram.append(clean_bigram_tokens)\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-steel",
   "metadata": {},
   "source": [
    "The dictionary is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bi=list(itertools.chain.from_iterable(new_bigram))\n",
    "fdist_bi = dict(nltk.FreqDist(list_bi))\n",
    "bi = {}\n",
    "for k,v in fdist_bi.items():\n",
    "    bi[\" \".join(k)] = fdist_bi[k]\n",
    "    bi[\" \".join(k)] = v\n",
    "\n",
    "#for x, y in bi.items():\n",
    "#    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-community",
   "metadata": {},
   "source": [
    "We have created a bar chart showing the 15 most frequent bigrams within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_sorted_bi = dict(sorted(bi.items(), key=lambda item: item[1], reverse=True))\n",
    "#print(fdist_sorted)\n",
    "out_bi = dict(itertools.islice(fdist_sorted_bi.items(), 15))\n",
    "\n",
    "df_bi = pd.DataFrame.from_dict(out_bi, orient='index').reset_index()\n",
    "df_bi = df_bi.rename(columns={'index':'bigrams', 0:'count'})\n",
    "#print(df)\n",
    "alt.renderers.enable('altair_viewer',inline=True)\n",
    "\n",
    "\n",
    "alt.Chart(\n",
    "    df_bi,\n",
    "    title = \"The 15 most frequent bigrams in the dataset\"\n",
    ").mark_bar().encode(\n",
    "    x=alt.X('count:Q'),\n",
    "    y=alt.Y('bigrams:N',sort='-x'),\n",
    "    color=alt.Color('count:Q',scale=alt.Scale(scheme=\"greens\"))\n",
    ").transform_window(\n",
    "    rank='rank(count)',\n",
    "    sort=[alt.SortField('count', order='descending')]\n",
    ").transform_filter(\n",
    "    (alt.datum.rank < 15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-pursuit",
   "metadata": {},
   "source": [
    "The word cloud is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=np.array(Image.open('grafici/number_two.jpg'))\n",
    "\n",
    "wordcloud_bi = WordCloud(\n",
    "            mask=mask,\n",
    "            width=mask.shape[1],\n",
    "            height=mask.shape[0],\n",
    "            background_color ='white', \n",
    "            min_word_length = 3,\n",
    "            max_words=400,\n",
    "            font_path = 'grafici/GothamMedium.ttf',\n",
    "            min_font_size = 10).generate_from_frequencies(bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-ordinance",
   "metadata": {},
   "source": [
    "Plot the word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Word Cloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_bi) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-collective",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-potential",
   "metadata": {},
   "source": [
    "We're interested in the \"full_text\" field, that it has been cleaned with specific functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_tri=0\n",
    "stop_words_tri = stopwords.words('english')\n",
    "new_trigram=[]\n",
    "for element in data:\n",
    "    data[index_tri]['full_text'] = data[index_tri]['full_text'].lower()                   # Put everything in lowercase\n",
    "    data[index_tri]['full_text'] = contractions.fix(data[index_tri]['full_text'])\n",
    "    #data[index_tri]['full_text'] = re.sub(\"\\'\\w+\", '', data[index_tri]['full_text'])     # Remove everything after '\n",
    "    data[index_tri]['full_text'] = remove_urls(data[index_tri]['full_text'])\n",
    "    data[index_tri]['full_text'] = remove_twitter_urls(data[index_tri]['full_text'])\n",
    "    data[index_tri]['full_text'] = remove_emoticons(data[index_tri]['full_text'])\n",
    "    data[index_tri]['full_text'] = remove_emoji(data[index_tri]['full_text'])\n",
    "    data[index_tri]['full_text'] = give_emoji_free_text(data[index_tri]['full_text'])\n",
    "    data[index_tri]['full_text'] = noamp(data[index_tri]['full_text'])                    # No amp with space\n",
    "    data[index_tri]['full_text'] = re.sub(\"#\\S+\", \" \",  data[index_tri]['full_text'])     # Remove hashtags\n",
    "    data[index_tri]['full_text'] = re.sub(\"@\\S+\", \" \",  data[index_tri]['full_text'])     # No mentions\n",
    "    data[index_tri]['full_text'] = data[index_tri]['full_text'].translate(str.maketrans('', '', string.punctuation)) # No puntuaction\n",
    "    data[index_tri]['full_text'] = data[index_tri]['full_text'].encode('ascii', 'ignore').decode() # No unicode\n",
    "    data[index_tri]['full_text'] = re.sub(\"^rt \", \" \", data[index_tri]['full_text'])      # No RT\n",
    "    data[index_tri]['full_text'] = re.sub(r'\\b\\d\\b', lambda x: num2words.num2words(int(x.group(0))), data[index_tri]['full_text'])\n",
    "    data[index_tri]['full_text'] = re.sub('\\s{2,}', \" \", data[index_tri]['full_text'])    # Remove big spaces\n",
    "    trigram_tokens=list(nltk.trigrams(nltk.word_tokenize(data[index_tri]['full_text'])))\n",
    "    #print(trigram_tokens)\n",
    "    clean_trigram_tokens = [gram for gram in trigram_tokens if not any(stop in gram for stop in stop_words_tri)]\n",
    "    new_trigram.append(clean_trigram_tokens)\n",
    "    index_tri=index_tri+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-camping",
   "metadata": {},
   "source": [
    "The dictionary is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tri=list(itertools.chain.from_iterable(new_trigram))\n",
    "fdist_tri = dict(nltk.FreqDist(list_tri))\n",
    "tri = {}\n",
    "for k,v in fdist_tri.items():\n",
    "    tri[\" \".join(k)] = fdist_tri[k]\n",
    "    tri[\" \".join(k)] = v\n",
    "\n",
    "#for x, y in tri.items():\n",
    "#    print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-diversity",
   "metadata": {},
   "source": [
    "We have created a bar chart showing the 15 most frequent trigrams within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-armor",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_sorted_tri = dict(sorted(tri.items(), key=lambda item: item[1], reverse=True))\n",
    "#print(fdist_sorted)\n",
    "out_tri = dict(itertools.islice(fdist_sorted_tri.items(), 15))\n",
    "\n",
    "df_tri = pd.DataFrame.from_dict(out_tri, orient='index').reset_index()\n",
    "df_tri = df_tri.rename(columns={'index':'trigrams', 0:'count'})\n",
    "#print(df)\n",
    "alt.renderers.enable('altair_viewer',inline=True)\n",
    "\n",
    "alt.Chart(\n",
    "    df_tri,\n",
    "    title = \"The 15 most frequent trigrams in the dataset\"\n",
    ").mark_bar().encode(\n",
    "    x=alt.X('count:Q'),\n",
    "    y=alt.Y('trigrams:N',sort='-x'),\n",
    "    color=alt.Color('count:Q',scale=alt.Scale(scheme=\"blues\"))\n",
    ").transform_window(\n",
    "    rank='rank(count)',\n",
    "    sort=[alt.SortField('count', order='descending')]\n",
    ").transform_filter(\n",
    "    (alt.datum.rank < 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-swimming",
   "metadata": {},
   "source": [
    "The word cloud is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tri=np.array(Image.open('grafici/number_three.jpg'))\n",
    "\n",
    "wordcloud_tri = WordCloud(\n",
    "            mask=mask_tri,\n",
    "            width=mask_tri.shape[1],\n",
    "            height=mask_tri.shape[0],\n",
    "            background_color ='white', \n",
    "            min_word_length = 3,\n",
    "            max_words=450,\n",
    "            font_path = 'grafici/GothamMedium.ttf',\n",
    "            min_font_size = 10).generate_from_frequencies(tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-survey",
   "metadata": {},
   "source": [
    "Plot the word cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Word Cloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_tri) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
