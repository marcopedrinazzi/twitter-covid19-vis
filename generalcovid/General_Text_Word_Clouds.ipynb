{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dependent-haiti",
   "metadata": {},
   "source": [
    "# General Text Word Clouds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-pollution",
   "metadata": {},
   "source": [
    "### wcloudimage.py  and wcloudimagenocovid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-input",
   "metadata": {},
   "source": [
    "In order to create the word clouds, we need the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "synthetic-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "#import sys\n",
    "import string\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import emoji\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-granny",
   "metadata": {},
   "source": [
    "Then we have defined the following functions to clean the tweets' text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incident-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return(result)\n",
    "\n",
    "def remove_twitter_urls(text):\n",
    "    clean = re.sub(r\"pic.twitter\\S+\", \"\",text)\n",
    "    return(clean)\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "\n",
    "def noamp(text):\n",
    "    clean = re.sub(\"&amp\", \" \",text)\n",
    "    return (clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-riding",
   "metadata": {},
   "source": [
    "To read the JSON file that has all the tweets, it is necessary to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-trigger",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('dataset/general_result.json', 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-direction",
   "metadata": {},
   "source": [
    "We're interested in the \"full_text\" field, that it has been cleaned with specific functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "index=0\n",
    "comment_words = ''\n",
    "stop_words = stopwords.words('english')\n",
    "for element in data:\n",
    "    data[index]['full_text'] = data[index]['full_text'].lower()                   # Put everything in lowercase\n",
    "    #data[index]['full_text'] = contractions.fix(data[index]['full_text'])\n",
    "    data[index]['full_text'] = re.sub(\"\\'\\w+\", '', data[index]['full_text'])      # Remove everything after '\n",
    "    data[index]['full_text'] = remove_urls(data[index]['full_text'])\n",
    "    data[index]['full_text'] = remove_twitter_urls(data[index]['full_text'])\n",
    "    data[index]['full_text'] = remove_emoticons(data[index]['full_text'])\n",
    "    data[index]['full_text'] = remove_emoji(data[index]['full_text'])\n",
    "    data[index]['full_text'] = give_emoji_free_text(data[index]['full_text'])\n",
    "    data[index]['full_text'] = noamp(data[index]['full_text'])                    # No amp with space\n",
    "    data[index]['full_text'] = re.sub(\"#\\S+\", \" \",  data[index]['full_text'])     # Remove hashtags\n",
    "    data[index]['full_text'] = re.sub(\"@\\S+\", \" \",  data[index]['full_text'])     # No mentions\n",
    "    data[index]['full_text'] = data[index]['full_text'].translate(str.maketrans('', '', string.punctuation)) # No puntuaction\n",
    "    data[index]['full_text'] = data[index]['full_text'].encode('ascii', 'ignore').decode() # No unicode\n",
    "    data[index]['full_text'] = re.sub(\"^rt \", \" \", data[index]['full_text'])      # No RT\n",
    "    data[index]['full_text'] = re.sub('\\s{2,}', \" \", data[index]['full_text'])    # Remove big spaces\n",
    "\n",
    "    \n",
    "    tokens=data[index]['full_text'].split()\n",
    "\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "    index=index+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-pursuit",
   "metadata": {},
   "source": [
    "Then the word cloud is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_covid= np.array(Image.open('grafici/face_mask.jpg'))\n",
    "\n",
    "wordcloud_covid = WordCloud(background_color ='white',\n",
    "            mask=mask_covid,\n",
    "            width=mask_covid.shape[1],\n",
    "            height=mask_covid.shape[0],\n",
    "            stopwords = stop_words,\n",
    "            normalize_plurals=False,\n",
    "            min_word_length = 3,\n",
    "            max_words=400,\n",
    "            font_path = 'grafici/GothamMedium.ttf',\n",
    "            min_font_size = 10).generate(comment_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-protocol",
   "metadata": {},
   "source": [
    "Plot the word cloud (WITH terms regarding Covid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-lunch",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the Word Cloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud_covid,interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-ivory",
   "metadata": {},
   "source": [
    "We have created a bar chart showing the 15 most used words within the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribuzione = wordcloud_covid.process_text(comment_words)\n",
    "#print(distribuzione)\n",
    "fdist_sorted = dict(sorted(distribuzione.items(), key=lambda item: item[1], reverse=True))\n",
    "#print(fdist_sorted)\n",
    "out = dict(itertools.islice(fdist_sorted.items(), 15))\n",
    "\n",
    "df = pd.DataFrame.from_dict(out, orient='index').reset_index()\n",
    "df = df.rename(columns={'index':'words', 0:'count'})\n",
    "#print(df)\n",
    "alt.renderers.enable('altair_viewer',inline=True)\n",
    "\n",
    "\n",
    "alt.Chart(\n",
    "    df,\n",
    "    title = \"Most 15 frequent words (Covid related) in the dataset\"\n",
    ").mark_bar().encode(\n",
    "    x=alt.X('count:Q'),\n",
    "    y=alt.Y('words:N',sort='-x'),\n",
    "    color=alt.Color('count:Q',scale=alt.Scale(scheme=\"greens\"))\n",
    ").transform_window(\n",
    "    rank='rank(count)',\n",
    "    sort=[alt.SortField('count', order='descending')]\n",
    ").transform_filter(\n",
    "    (alt.datum.rank < 15)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-enhancement",
   "metadata": {},
   "source": [
    "We have also created a word cloud without terms regarding Covid, so we have filtered these words from the \"full_text\" field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_nocovid=0\n",
    "comment_words_nocovid = ''\n",
    "stop_words_nocovid = stopwords.words('english') + [\"2019ncov\", \"covid19\", \"indiafightscorona\", \"coronavirus\", \"sarscov2\", \"corona\", \"covid_19\", \"covid\",\"coronavirusoutbreak\",\"covid2019\", \"virus\", \"covid__19\", \"covidãƒ¼19\", \"covid-19\", \"pandemic\", \"coronaviruspandemic\", \"covid19outbreak\",\"pandemiac\"]\n",
    "for element in data:\n",
    "    data[index_nocovid]['full_text'] = data[index_nocovid]['full_text'].lower()                # Put everything in lowercase\n",
    "    #data[index]['full_text'] = contractions.fix(data[index]['full_text'])\n",
    "    data[index_nocovid]['full_text'] = re.sub(\"\\'\\w+\", '', data[index_nocovid]['full_text'])   # Remove everything after '\n",
    "    data[index_nocovid]['full_text'] = remove_urls(data[index_nocovid]['full_text'])\n",
    "    data[index_nocovid]['full_text'] = remove_twitter_urls(data[index_nocovid]['full_text'])\n",
    "    data[index_nocovid]['full_text'] = remove_emoticons(data[index_nocovid]['full_text'])\n",
    "    data[index_nocovid]['full_text'] = remove_emoji(data[index_nocovid]['full_text'])\n",
    "    data[index_nocovid]['full_text'] = give_emoji_free_text(data[index_nocovid]['full_text'])\n",
    "    data[index_nocovid]['full_text'] = noamp(data[index_nocovid]['full_text'])                 # No amp with space\n",
    "    data[index_nocovid]['full_text'] = re.sub(\"#\\S+\", \" \",  data[index_nocovid]['full_text'])  # Remove hashtags\n",
    "    data[index_nocovid]['full_text'] = re.sub(\"@\\S+\", \" \",  data[index_nocovid]['full_text'])  # No mentions\n",
    "    data[index_nocovid]['full_text'] = data[index_nocovid]['full_text'].translate(str.maketrans('', '', string.punctuation)) # No puntuaction\n",
    "    data[index_nocovid]['full_text'] = data[index_nocovid]['full_text'].encode('ascii', 'ignore').decode() # No unicode\n",
    "    data[index_nocovid]['full_text'] = re.sub(\"^rt \", \" \", data[index_nocovid]['full_text'])   # No RT\n",
    "    data[index_nocovid]['full_text'] = re.sub('\\s{2,}', \" \", data[index_nocovid]['full_text']) # Remove big spaces\n",
    "\n",
    "    \n",
    "    tokens_nocovid=data[index_nocovid]['full_text'].split()\n",
    "\n",
    "    comment_words_nocovid += \" \".join(tokens_nocovid)+\" \"\n",
    "    index_nocovid=index_nocovid+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-banks",
   "metadata": {},
   "source": [
    "The word cloud is generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-belgium",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mask= np.array(Image.open('grafici/twitter.jpg'))\n",
    "\n",
    "wordcloud = WordCloud(background_color ='white',\n",
    "            mask=mask,\n",
    "            width=mask.shape[1],\n",
    "            height=mask.shape[0],          \n",
    "            stopwords = stop_words_nocovid, \n",
    "            normalize_plurals=False,\n",
    "            min_word_length = 3,\n",
    "            font_path = 'grafici/GothamMedium.ttf',\n",
    "            min_font_size = 10).generate(comment_words_nocovid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-tucson",
   "metadata": {},
   "source": [
    "Plot the word cloud (WITHOUT terms regarding Covid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Word Cloud image                        \n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\") \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-marina",
   "metadata": {},
   "source": [
    "Then the bar chart showing the 15 top words within the dataset, always without considering terms regarding Covid, is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = wordcloud.process_text(comment_words_nocovid)\n",
    "#print(distribuzione)\n",
    "fdist_sorted_nocovid = dict(sorted(dis.items(), key=lambda item: item[1], reverse=True))\n",
    "#print(fdist_sorted)\n",
    "out_nocovid = dict(itertools.islice(fdist_sorted_nocovid.items(), 15))\n",
    "\n",
    "df_nocovid = pd.DataFrame.from_dict(out_nocovid, orient='index').reset_index()\n",
    "df_nocovid = df_nocovid.rename(columns={'index':'words', 0:'count'})\n",
    "alt.renderers.enable('altair_viewer',inline=True)\n",
    "\n",
    "\n",
    "alt.Chart(\n",
    "    df_nocovid,\n",
    "    title = \"Most 15 frequent words (NOT Covid related) in the dataset\"\n",
    ").mark_bar().encode(\n",
    "    x=alt.X('count:Q'),\n",
    "    y=alt.Y('words:N',sort='-x'),\n",
    "    color=alt.Color('count:Q',scale=alt.Scale(scheme=\"blues\"))\n",
    ").transform_window(\n",
    "    rank='rank(count)',\n",
    "    sort=[alt.SortField('count', order='descending')]\n",
    ").transform_filter(\n",
    "    (alt.datum.rank < 15)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
